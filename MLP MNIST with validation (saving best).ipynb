{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', train=True, transform=transform)\n",
    "testset = datasets.MNIST('~/.pytorch/MNIST_data/', train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subsetting a validation set\n",
    "\n",
    "valid_pct = 0.2\n",
    "train_length = len(trainset)\n",
    "valid_length = int(np.floor(train_length*valid_pct))\n",
    "\n",
    "idx = list(range(train_length))\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "train_idx, valid_idx = idx[valid_length:], idx[:valid_length] # First valid_length indexes for validation, rest for training\n",
    "\n",
    "# Create samplers\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=train_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=valid_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NN architecture\n",
    "\n",
    "class Arc(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes, p=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden = nn.ModuleList([nn.Linear(input_size,hidden_sizes[0])])\n",
    "        hidden_layers = zip(hidden_sizes[:-1],hidden_sizes[1:])\n",
    "        self.hidden.extend(nn.Linear(a,b) for a,b in hidden_layers)\n",
    "        \n",
    "        self.output = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        for each in self.hidden:\n",
    "            x = self.dropout(F.relu(each(x)))\n",
    "        x = self.output(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arc(\n",
       "  (hidden): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): Linear(in_features=256, out_features=64, bias=True)\n",
       "  )\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Arc(784,10,[256,64])\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 Train_loss: 1.2503 Valid_loss: 0.2058\n",
      "Validation loss decreased from inf to 0.2058. Saving model\n",
      "Epoch: 2/30 Train_loss: 1.2229 Valid_loss: 0.1764\n",
      "Validation loss decreased from 0.2058 to 0.1764. Saving model\n",
      "Epoch: 3/30 Train_loss: 1.2610 Valid_loss: 0.2238\n",
      "Epoch: 4/30 Train_loss: 1.2746 Valid_loss: 0.2309\n",
      "Epoch: 5/30 Train_loss: 1.3611 Valid_loss: 0.2589\n",
      "Epoch: 6/30 Train_loss: 1.3908 Valid_loss: 0.4305\n",
      "Epoch: 7/30 Train_loss: 1.4073 Valid_loss: 0.2625\n",
      "Epoch: 8/30 Train_loss: 1.4174 Valid_loss: 0.2539\n",
      "Epoch: 9/30 Train_loss: 1.3824 Valid_loss: 0.2466\n",
      "Epoch: 10/30 Train_loss: 1.4014 Valid_loss: 0.2537\n",
      "Epoch: 11/30 Train_loss: 1.4149 Valid_loss: 0.2798\n",
      "Epoch: 12/30 Train_loss: 1.3936 Valid_loss: 0.2584\n",
      "Epoch: 13/30 Train_loss: 1.3928 Valid_loss: 0.2849\n",
      "Epoch: 14/30 Train_loss: 1.4203 Valid_loss: 0.2837\n",
      "Epoch: 15/30 Train_loss: 1.4031 Valid_loss: 0.2871\n",
      "Epoch: 16/30 Train_loss: 1.3814 Valid_loss: 0.2794\n",
      "Epoch: 17/30 Train_loss: 1.4332 Valid_loss: 0.2869\n",
      "Epoch: 18/30 Train_loss: 1.4176 Valid_loss: 0.2836\n",
      "Epoch: 19/30 Train_loss: 1.4007 Valid_loss: 0.2742\n",
      "Epoch: 20/30 Train_loss: 1.4191 Valid_loss: 0.2788\n",
      "Epoch: 21/30 Train_loss: 1.4122 Valid_loss: 0.3019\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "min_valid_loss = np.Inf\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    # Training loop\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "    # Validation loop\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for images, labels in valid_loader:\n",
    "            log_ps = model(images)\n",
    "            ps = torch.exp(log_ps)\n",
    "            loss = criterion(log_ps, labels)\n",
    "            valid_loss += loss.item()*images.size(0)\n",
    "        model.train()\n",
    "    \n",
    "    avg_valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    avg_train_loss = train_loss/len(train_loader.dataset)\n",
    "    print('Epoch: {}/{}'.format(e+1, epochs),\n",
    "         'Train_loss: {:.4f}'.format(avg_train_loss),\n",
    "         'Valid_loss: {:.4f}'.format(avg_valid_loss))\n",
    "    \n",
    "    if avg_valid_loss <= min_valid_loss:\n",
    "        torch.save(model.state_dict(), 'Best_model.pth')\n",
    "        print('Validation loss decreased from {:.4f} to {:.4f}. Saving model'.format(min_valid_loss, avg_valid_loss))\n",
    "        min_valid_loss = avg_valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model = model.load_state_dict('Best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking accuracy on test set\n",
    "test_loss = 0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval()\n",
    "for images, labels in test_loader:\n",
    "    log_ps = model(images)\n",
    "    ps = torch.exp(log_ps)\n",
    "    loss = criterion(log_ps, labels)\n",
    "    test_loss += loss.item()*labels.size(0)\n",
    "    _, pred = torch.max(log_ps, 1)\n",
    "    correct = np.squeeze(pred.eq(labels.data.view_as(pred)))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        label = labels.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "print('Test_loss: {}'.format(test_loss/len(test_loader.dataset)))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai-3.7)",
   "language": "python",
   "name": "fastai-3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
