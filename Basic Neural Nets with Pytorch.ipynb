{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets,transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,),(0.5,))])\n",
    "\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset,batch_size=32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2611ba89780>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "inputs = images.view(images.shape[0],-1)\n",
    "\n",
    "n_hidden = 256\n",
    "n_output = 10\n",
    "\n",
    "W1 = torch.randn(inputs.shape[1], n_hidden)\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "B1 = torch.randn(n_hidden)\n",
    "B2 = torch.randn(n_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1 = sigmoid(torch.mm(inputs, W1) + B1)\n",
    "\n",
    "out = torch.mm(h1, W2) + B2\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim = 1).view(inputs.shape[0],1)\n",
    "\n",
    "probabilities = softmax(out)\n",
    "\n",
    "print(probabilities.shape)\n",
    "\n",
    "print(probabilities.sum(dim = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Pytorch nn module to create a network with 784 input units, a hidden layer with 128 units and a ReLU activation, then a hidden layer with 64 units and a ReLU activation, and finally an output layer with a softmax activation as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden1 = nn.Linear(784,128)\n",
    "        self.hidden2 = nn.Linear(128,64)\n",
    "        self.output = nn.Linear(64,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (hidden2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the weights and biases are automatically initiallized for us through random sampling from a normal distribution. But we can customize how they are initialized. To access the, we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 784])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(model.hidden1.weight.shape)\n",
    "print(model.hidden1.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting biases to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden1.bias.data.fill_(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting weights from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0463,  0.0711, -0.0220,  ...,  0.0634, -0.0601, -0.1328],\n",
       "        [ 0.0379, -0.1220, -0.0541,  ..., -0.0076, -0.0949, -0.0448],\n",
       "        [ 0.0187,  0.0884,  0.0929,  ...,  0.0276, -0.0088,  0.0464],\n",
       "        ...,\n",
       "        [ 0.1660,  0.1694, -0.0568,  ...,  0.0332, -0.0962, -0.0767],\n",
       "        [ 0.0365,  0.0888, -0.1164,  ...,  0.0390,  0.0652,  0.0248],\n",
       "        [-0.0239,  0.1637,  0.1999,  ..., -0.2524,  0.0728, -0.0876]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden1.weight.data.normal_(mean=0., std=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].view(1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0580, 0.1028, 0.1913, 0.0470, 0.0583, 0.0474, 0.0780, 0.0951, 0.1498,\n",
       "         0.1723]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = model.forward(images[0].view(1,-1))\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): Softmax()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0970, 0.1209, 0.1022, 0.0958, 0.0959, 0.0977, 0.1005, 0.0912, 0.0979,\n",
       "         0.1010]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [256,64]\n",
    "output_size = 10\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size,hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)\n",
    "\n",
    "model.forward(images[0].view(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0327, -0.0133,  0.0229,  0.0318, -0.0150, -0.0180, -0.0088, -0.0064,\n",
       "        -0.0348, -0.0272, -0.0195, -0.0287, -0.0098,  0.0225, -0.0346,  0.0147,\n",
       "        -0.0180,  0.0289,  0.0007, -0.0017,  0.0304, -0.0226, -0.0154, -0.0148,\n",
       "        -0.0083,  0.0308, -0.0109,  0.0074, -0.0095, -0.0213,  0.0181,  0.0281,\n",
       "         0.0225, -0.0031, -0.0046,  0.0253,  0.0271, -0.0240, -0.0125,  0.0265,\n",
       "         0.0092, -0.0289,  0.0157,  0.0142,  0.0142,  0.0172,  0.0275, -0.0300,\n",
       "         0.0199, -0.0274,  0.0032,  0.0150, -0.0037, -0.0257, -0.0090, -0.0285,\n",
       "        -0.0226,  0.0108,  0.0088, -0.0249, -0.0186,  0.0351,  0.0075,  0.0235,\n",
       "        -0.0324,  0.0056,  0.0253, -0.0143, -0.0221, -0.0236, -0.0265,  0.0025,\n",
       "         0.0343,  0.0265,  0.0327,  0.0210, -0.0062, -0.0307, -0.0078, -0.0018,\n",
       "        -0.0024, -0.0146,  0.0335, -0.0005,  0.0062,  0.0261, -0.0099,  0.0055,\n",
       "        -0.0163, -0.0089,  0.0074,  0.0214, -0.0206, -0.0206, -0.0028, -0.0206,\n",
       "        -0.0277,  0.0041, -0.0097,  0.0289,  0.0030, -0.0265,  0.0040, -0.0307,\n",
       "        -0.0120,  0.0116,  0.0346, -0.0104,  0.0021,  0.0217,  0.0053,  0.0214,\n",
       "        -0.0275, -0.0128, -0.0156,  0.0346,  0.0079,  0.0111, -0.0281,  0.0184,\n",
       "         0.0123, -0.0349, -0.0329, -0.0335,  0.0229,  0.0286,  0.0247, -0.0242,\n",
       "         0.0163, -0.0187, -0.0057,  0.0294,  0.0097, -0.0259, -0.0313, -0.0329,\n",
       "        -0.0357, -0.0047,  0.0189, -0.0039, -0.0353, -0.0085,  0.0234, -0.0239,\n",
       "         0.0090, -0.0095, -0.0296,  0.0338,  0.0097, -0.0086, -0.0181,  0.0346,\n",
       "         0.0043, -0.0276, -0.0217,  0.0028, -0.0113, -0.0026, -0.0216,  0.0203,\n",
       "         0.0137, -0.0230,  0.0028,  0.0344,  0.0200, -0.0307, -0.0045, -0.0081,\n",
       "         0.0115,  0.0279, -0.0164,  0.0193,  0.0258,  0.0021, -0.0325,  0.0187,\n",
       "        -0.0260, -0.0236,  0.0057, -0.0354, -0.0250, -0.0342, -0.0059,  0.0180,\n",
       "        -0.0100, -0.0104,  0.0306, -0.0135,  0.0016, -0.0221, -0.0078,  0.0069,\n",
       "         0.0105, -0.0225, -0.0302, -0.0352, -0.0341, -0.0179,  0.0222,  0.0176,\n",
       "         0.0105,  0.0023, -0.0005,  0.0332,  0.0014,  0.0179,  0.0099,  0.0125,\n",
       "        -0.0068,  0.0061,  0.0094,  0.0293,  0.0321, -0.0057, -0.0301,  0.0164,\n",
       "         0.0343,  0.0278, -0.0160,  0.0293,  0.0343, -0.0005,  0.0240, -0.0199,\n",
       "        -0.0089, -0.0138,  0.0069, -0.0323, -0.0209,  0.0041, -0.0024, -0.0249,\n",
       "        -0.0286, -0.0311,  0.0227,  0.0028, -0.0332,  0.0030, -0.0157,  0.0251,\n",
       "         0.0157, -0.0339,  0.0127,  0.0355,  0.0218,  0.0065, -0.0255, -0.0203,\n",
       "        -0.0011,  0.0016,  0.0240, -0.0297, -0.0048, -0.0086, -0.0190, -0.0029])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also name the layers in nn.Sequential using OrderedDicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "                      ('softmax', nn.Softmax(dim=1))]))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=256, bias=True) Linear(in_features=784, out_features=256, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model[0], model.fc1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai-3.7)",
   "language": "python",
   "name": "fastai-3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
