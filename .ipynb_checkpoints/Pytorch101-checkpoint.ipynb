{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just some basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Pytorch Library\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a seed helps in the reproducibility of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f3de0827b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to create a tensor in Pytorch? It's pretty simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1468,  0.7861,  0.9468, -1.1143],\n",
       "        [ 1.6908, -0.8948, -0.3556,  1.2324]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((2,4)) # Pass the dimensions of the tensor we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.5827, -0.3246,  1.9264],\n",
      "         [-0.3300,  0.1984,  0.7821]],\n",
      "\n",
      "        [[ 1.0391, -0.7245, -0.1354],\n",
      "         [ 0.7471,  0.6118,  1.8678]],\n",
      "\n",
      "        [[ 2.5116, -1.2548,  0.8165],\n",
      "         [-1.0654, -1.6370,  0.1577]],\n",
      "\n",
      "        [[ 0.3957, -1.3677, -0.1007],\n",
      "         [ 0.2370,  0.6327, -0.0917]]])\n",
      "tensor([[0.2674, 0.4990, 0.7447],\n",
      "        [0.7213, 0.4414, 0.5550]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.randn((4,2,3))) \n",
    "print(torch.rand((2,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a tensor of same shape as another tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((2,3))\n",
    "b = torch.randn_like(a) #_like implies that the new tensor will have same shape as the tensor passed to it\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** It's a great practice to keep an eye on the shape of the tensors when we are working with them, because most of the errors we face later (when doing complex NN architectures) are related to the proper shapes of tensors not being provided for a function down the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily pass tensors through functions. For example -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a sigmoid function\n",
    "def activation(x):\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2202,  0.7588, -0.1857],\n",
       "        [-0.8985,  0.6095,  0.2240]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7721, 0.6811, 0.4537],\n",
       "        [0.2894, 0.6478, 0.5558]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really helpful when we create complex neural networks, with lots of different activation functions being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7173,  0.0074, -0.1595, -0.7964, -0.3788]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.randn((1,5))\n",
    "x = torch.randn_like(weights)\n",
    "weights * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple multiplication gives us element wise multiplication. To get dot product, we can do a number of things, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.0446)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(weights * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.0446)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(weights * x).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But when we will do deep learning, we will require to do huge matrix/tensor multiplications and it will be difficult to keep track of dimensions, so we will use torch.mm where mm stands for matrix multiplication instead. Moreover, these functions are more efficient in memory usage and have been implemented using modern libraries which accelerate the multiplication process as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 5], m2: [1 x 5] at c:\\a\\w\\1\\s\\tmp_conda_3.7_110509\\conda\\conda-bld\\pytorch_1544094576194\\work\\aten\\src\\th\\generic/THTensorMath.cpp:940",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-55f565b2d224>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 5], m2: [1 x 5] at c:\\a\\w\\1\\s\\tmp_conda_3.7_110509\\conda\\conda-bld\\pytorch_1544094576194\\work\\aten\\src\\th\\generic/THTensorMath.cpp:940"
     ]
    }
   ],
   "source": [
    "torch.mm(weights, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's this error? As we can see that there is a size mismatch because for multiplying (a x b) and (c x d) matrix, both b and c should have same value (Look up matrix multiplication in general as to why if this is not clear). How to do that? Transpose!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0446]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(weights, x.transpose(0,1)) # 0,1 are the dimensions being switched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are better ways to tackle this problem, by using either of:\n",
    "\n",
    "* `weights.reshape(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)` sometimes, and sometimes a clone, as in it copies the data to another part of memory.\n",
    "* `weights.resize_(a, b)` returns the same tensor with a different shape. However, if the new shape results in fewer elements than the original tensor, some elements will be removed from the tensor (but not from memory). If the new shape results in more elements than the original tensor, new elements will be uninitialized in memory. Here I should note that the underscore at the end of the method denotes that this method is performed **in-place**. Here is a great forum thread to [read more about in-place operations](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) in PyTorch.\n",
    "* `weights.view(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I personally favour using .view() because it returns an error if we are trying to change the number of elements in a tensor when trying to reshape it and it keeps me from making such mistakes. But feel free to use whichever of them makes you comfortable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0446]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(weights, x.view(5,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And next we can pass it through the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1146]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation(torch.mm(weights, x.view(5,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Know what we did here? We created a simple neural network! (Albeit only the forward pass, but more on that later).\n",
    "Next let's introduce a bias term as well.\n",
    "\n",
    "Actually let's create a simple neural net with one hidden layer from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7)\n",
    "\n",
    "x = torch.randn((1,3)) # We will have three inputs, x1, x2 and x3\n",
    "\n",
    "input_size = x.shape[1] # Only take the number of columns (3 here)\n",
    "hidden_size = 2 # Let's use 2 neurons in the hidden layer\n",
    "output_size = 1 # We only want a single value as output\n",
    "\n",
    "weights_for_hidden_layer = torch.randn(input_size, hidden_size) # The weights from input to hidden layer\n",
    "weights_for_output_layer = torch.randn(hidden_size, output_size) # The weights from hidden to output layer\n",
    "\n",
    "bias_for_hidden_layer = torch.randn(1,hidden_size) # we need to add bias to each neuron in the hidden layer\n",
    "bias_for_output_layer = torch.randn(1,output_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3171]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Network creation\n",
    "\n",
    "hidden_layer_output = activation(\n",
    "    torch.mm(x, weights_for_hidden_layer) \n",
    "    + bias_for_hidden_layer)\n",
    "\n",
    "output = activation(\n",
    "    torch.mm(hidden_layer_output, weights_for_output_layer) \n",
    "    + bias_for_output_layer)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is how we create a simple neural network in Pytorch. Simple, right?\n",
    "\n",
    "The trick is to take on one thing at a time and not trying to do everything at once. Do matrix multiplication first, see if its shape is right, then add the biases, then pass it through activation function. Repeat. Baby steps!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai-3.7)",
   "language": "python",
   "name": "fastai-3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
